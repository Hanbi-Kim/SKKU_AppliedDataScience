# SUNGKYUNKWAN University M.S Applied Data Science  <br/><br/>
## 3rd Semester
### ADS5019 Deep Learning
Survey Report: Semi-Supervised Learning in Image Classification.<br/>
[Report(ENG)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5019_%EB%94%A5%EB%9F%AC%EB%8B%9D/Survey%20Report%20-%20Semi%20Supervised%20Learning%20in%20Image%20Classification.pdf)

<br/><br/>

## 2nd Semester
### ADS5006 Machine Learning 
Comparing some machine learning model performance on detecting VF/VT(Heart Diseaes) by ECG leads data set.<br/>
[PPT(KR)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5006_%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5%ED%8A%B9%EB%A1%A0/ADS5006_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C_%EB%B6%80%EC%A0%95%EB%A7%A5%ED%8C%90%EB%B3%84%ED%95%98%EA%B8%B0_%EA%B9%80%ED%95%9C%EB%B9%84.pdf) |
[Codes(KR)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5006_%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5%ED%8A%B9%EB%A1%A0/ADS5006_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C_%EB%B6%80%EC%A0%95%EB%A7%A5%ED%8C%90%EB%B3%84%ED%95%98%EA%B8%B0_%EA%B9%80%ED%95%9C%EB%B9%84.ipynb)<br/>

<br/><br/>

### ADS5037 Reinforcement Learning
Reviewing journal, [Deep Q-Learning from Demonstration](https://arxiv.org/pdf/1704.03732.pdf) and apply theroy to python library Cartpole-V1.<br/>
[PPR(KR)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5037_%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/ADS5037_%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5_%EA%B8%B0%EB%A7%90%EA%B3%BC%EC%A0%9C_%EA%B9%80%ED%95%9C%EB%B9%84.pdf)   |
[Codes(KR)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5037_%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/agent_1.py)<br/>
<br/><br/>

## 1st Semester
### ADS5004 Programming for Data Analysis
By collecting weather data from Korea Weather Forecast, analyze important facters affecting dust(PM-10) and predict PM-10 level on the day after based on weather data set.<br/>
[PPT(KR)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5004_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%96%B8%EC%96%B4/ADS5004_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%96%B8%EC%96%B4_%EA%B8%B0%EB%A7%90%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.pdf) |
[Codes(KR)](https://github.com/Hanbi-Kim/SKKU-Applied-Data-Science/blob/main/ADS5004_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%96%B8%EC%96%B4/ADS5004_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%96%B8%EC%96%B4_%EA%B8%B0%EB%A7%90%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.ipynb)<br/>

<br/><br/>

### Reseach Paper 
Computer Vision / Image Classification / Semi-supervised Learning 

### **References:**

- Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. [[pdf]](https://arxiv.org/abs/1804.09170) [[code]](https://github.com/brain-research/realistic-ssl-evaluation)
    - Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, Ian J. Goodfellow. *NeurIPS 2018*
- Semi-Supervised Learning Literature Survey. [[pdf]](http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf)
    - Xiaojin Zhu. *2008*
- An Overview of Deep Semi-Supervised Learning. [[pdf]](https://arxiv.org/abs/2006.05278)
    - Yassine Ouali, Céline Hudelot, Myriam Tami. *2020*
- A survey on semi-supervised learning. [[pdf]](https://link.springer.com/content/pdf/10.1007/s10994-019-05855-6.pdf)
    - Jesper E Van Engelen, Holger H Hoos. *2020*
- A Survey on Deep Semi-Supervised Learning [[pdf]](https://arxiv.org/pdf/2103.00550.pdf)
    - Xiangli Yang, Zixing Song, Irwin King. *2021*

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c9e597ff-714f-4909-81df-bc978b67ac78/Untitled.png)

### DSSL Method Categories:

1. Generative Models
2. Consistency Regularization Models
3. Graph-based Models
4. Pseudo-labelling Models
5. Hybrid Models

Personal research interests focus on Hybrid models and it is based on consistency regularization models and pseudo-labelling models. 

Some of consistency regularization models and pseudo-labelling models still used as model comparison and evaluation standard in hybrid model research papers.   

### Comparison of SSL Algorithms including consistency regularization.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/525714e2-d4ac-48f3-afc7-2ad9accd1147/Untitled.png)

Table from FixMatch research paper

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4cbccdf0-77a2-4608-b825-e8a4052aeddb/Untitled.png)

Table from PaperWithCodes

# **Consistency Regularization**

1. Ladder Network
    1. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko, “Semi-supervised learning with ladder networks,” in NIPS, 2015, pp. 3546–3554. (Cited by 1211)
    2. Pezeshki, L. Fan, P. Brakel, A. C. Courville, and Y. Bengio, “Deconstructing the ladder network architecture,” in ICML, ser. JMLR Workshop and Conference Proceedings, vol. 48. JMLR.org, 2016, pp. 2368–2376. (Cited by 103)
2. Π Model
    1. Sajjadi, M. Javanmardi, and T. Tasdizen, “Regularization with stochastic transformations and perturbations for deep semi-supervised learning,” in NIPS, 2016, pp. 1163–1171. (Cited by 518) [[pdf](https://proceedings.neurips.cc/paper/2016/file/30ef30b64204a3088a26bc2e6ecf7602-Paper.pdf)]
3. Temporal Ensembling
    1. Laine and T. Aila, “Temporal ensembling for semi-supervised learning,” in ICLR. OpenReview.net, 2017. (Cited by 1268) [[pdf](https://arxiv.org/pdf/1610.02242.pdf))]
4. Mean Teacher
    1. Tarvainen and H. Valpola, “Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,” in NIPS, 2017, pp. 1195–1204. (Cited by 1640) [[pdf](https://arxiv.org/pdf/1703.01780.pdf)]
5. VAT (Virtual Adversarial Training)
    1. Miyato, S. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial training: A regularization method for supervised and semi-supervised learning,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 8, pp. 1979–1993, 2019. (Cited by 1367) [[pdf](https://arxiv.org/pdf/1704.03976.pdf)]
6. Dual Student
    1. Ke, D. Wang, Q. Yan, J. S. J. Ren, and R. W. H. Lau, “Dual student: Breaking the limits of the teacher in semi-supervised learning,” in ICCV. IEEE, 2019, pp. 6727–6735. (Cited by 50)
7. SWA (Stochastic Weight Averaging)
    1. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson, “Averaging weights leads to wider optima and better generalization,” in UAI. AUAI Press, 2018, pp. 876–885. (Cited by 488)
8. VAdD (Virtual Adversial Dropout)
    1. Park, J. Park, S. Shin, and I. Moon, “Adversarial dropout for supervised and semi-supervised learning,” in AAAI. AAAI Press, 2018, pp. 3917–3924. (Cited by 116)
9. WCP (Worse-case Perturbation)
    1. Zhang and G. Qi, “WCP: worst-case perturbations for semi-supervised deep learning,” in CVPR. IEEE, 2020, pp. 3911–3920. (Cited by 14)
10. UDA (Unsupervised Data Augmentation)
    1. Xie, Z. Dai, E. H. Hovy, T. Luong, and Q. Le, “Unsupervised data augmentation for consistency training,” in NeurIPS, 2020. (Cited by 593)
    

# **Pseudo-Labelling**

## **Disagreement-based Models**

1. Deep Co-training
    1. Blum and T. M. Mitchell, “Combining labeled and unlabeled data with co-training,” in COLT. ACM, 1998, pp. 92–100. (cited by 6655)
2. Tri-Net
    1. Chen, W. Wang, W. Gao, and Z. Zhou, “Tri-net for semi-supervised deep learning,” in IJCAI. ijcai.org, 2018, pp. 2014– 2020. (Cited by 65)
    

## **Self-training Models**

1. Pseudo-label
    1. D-H. Lee, “Pseudo-label: The simple and efficient semi supervised learning method for deep neural networks,” in Workshop on challenges in representation learning, ICML, vol. 3, no. 2, 2013. (Cited by 1524)
2. Noisy Student
    1. Xie, M. Luong, E. H. Hovy, and Q. V. Le, “Self-training with noisy student improves imagenet classification,” in CVPR. IEEE, 2020, pp. 10 684–10 695. (Cited by 857)
3. S4­L (Self-supervised Semi-supervised Learning)
    1. Beyer, X. Zhai, A. Oliver, and A. Kolesnikov, “S4L: self-supervised semi-supervised learning,” in ICCV. IEEE, 2019, pp. 1476–1485. (Cited by 368)
4. MPL (Meta Pseudo Labels)
    1. Pham, Q. Xie, Z. Dai, and Q. V. Le, “Meta pseudo labels,” CoRR, vol. abs/2003.10580, 2020. (Cited by 148)
5. EnAET (Ensemble of Auto-Encoding Transformations)
    1. Wang, D. Kihara, J. Luo, and G. Qi, “Enaet: A self-trained framework for semi-supervised and supervised learning with ensemble transformations,” IEEE Trans. Image Process., vol. 30, pp. 1639–1647, 2021. (Cited by 9)
6. SimCLRv2 (Simple framework for Contrastive Learning of visual Representation )
    1. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big self-supervised models are strong semi-supervised learners,” in NeurIPS, 2020. (Cited by 507)  [[pdf](https://arxiv.org/pdf/2006.10029.pdf)]
    
    MOCO →facebook 
    

# **Hybrid Method**

1. ICT (Interpolation Consistency Training)
    1. Verma, A. Lamb, J. Kannala, Y. Bengio, and D. Lopez-Paz, “Interpolation consistency training for semi-supervised learning,” in IJCAI. ijcai.org, 2019, pp. 3635–3641. (Cited by 262)
2. MixMatch
    1. Berthelot, N. Carlini, I. J. Goodfellow, N. Papernot, A. Oliver, and C. Raffel, “Mixmatch: A holistic approach to semi-supervised learning,” in NeurIPS, 2019, pp. 5050–5060. (Cited by 1003) [[pdf](https://arxiv.org/pdf/1905.02249.pdf)]
3. ReMixMatch
    1. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel, “Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring,” in ICLR. OpenReview.net, 2020. (Cited by 310) [[pdf](https://arxiv.org/pdf/1911.09785.pdf)]
4. DivideMix
    1. Li, R. Socher, and S. C. H. Hoi, “Dividemix: Learning with noisy labels as semi-supervised learning,” in ICLR. OpenReview.net, 2020 (Cited by 194)
5. FixMatch
    1. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. Raffel, E. D. Cubuk, A. Kurakin, and C. Li, “Fixmatch: Simplifying semi-supervised learning with consistency and confidence,” in NeurIPS, 2020. (Cited by 595)

## Additional Research Papers

1. FlexMatch
    1. Zhang, Bowen, et al. "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling." *Advances in Neural Information Processing Systems* 34 (2021). (Cited by 3) [[pdf](https://proceedings.neurips.cc/paper/2021/file/995693c15f439e3d189b06e89d145dd5-Paper.pdf)]
2. LaplaceNet
    1. Sellars, Philip, Angelica I. Aviles-Rivero, and Carola-Bibiane Schönlieb. "LaplaceNet: A Hybrid Energy-Neural Model for Deep Semi-Supervised Classification." *arXiv preprint arXiv:2106.04527* (2021).
3. Dash
    1. Xu, Yi, et al. "Dash: Semi-supervised learning with dynamic thresholding." *International Conference on Machine Learning*. PMLR, 2021. (Cited by 3) 
4. Semi-MMDC
    1. Lerner, Boaz, Guy Shiran, and Daphna Weinshall. "Boosting the Performance of Semi-Supervised Learning with Unsupervised Clustering." *arXiv preprint arXiv:2012.00504* (2020). (Cited by 1) 

* Citation = Google Scholar updated on 2021 Dec 26

# **Challenges and Future Directions**

1. Theoretical Analysis
2. Interpolation of domain knowledge
3. Learning with noisy labels
4. Imbalanced semi-supervised learning
5. Robust semi-supervised learning
6. Safe semi-supervised learning

## 1. Theoretical Analysis

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/428f519e-c577-430a-909f-ffc31887207b/Untitled.png)

- Only single weight used for training

### Related research:

1. How to use a different weight for every unlabeled example (Influence Function).
    1. Z. Ren, R. A. Yeh, and A. G. Schwing, “Not all unlabeled data are equal: Learning to weight data in semi-supervised learning,” in NeurIPS, 2020. (Cited by 19) [[pdf](https://arxiv.org/pdf/2007.01293.pdf)]
2. How loss geometry interacts with training process.
    1. [B. Athiwaratkun, M. Finzi, P. Izmailov, and A. G. Wilson, “There are many consistent explanations of unlabeled data: Why you should average,” in ICLR. OpenReview.net, 2019. (Cited by 149)](https://arxiv.org/pdf/1806.05594.pdf)
3. Explores the effects, limitation and interaction of data augmentation and labeled dataset size on pre-training and self-training.
    1. [B. Zoph, G. Ghiasi, T. Lin, Y. Cui, H. Liu, E. D. Cubuk, and Q. Le, “Rethinking pre-training and self-training,” in NeurIPS, 2020.(Cited by 182)](https://arxiv.org/pdf/1806.05594.pdf)
4. Analyzes the property of the consistency regularization methods when data instances lie in the neighbourhood of low-dimensional manifolds. 
    1. [A. Ghosh and A. H. Thiery, “On data-augmentation and consistency-based semi-supervised learning,” in ICLR. OpenReview.net, 2021. (Cited by 3)](https://arxiv.org/pdf/2101.06967.pdf)

### Approach:

1. Using soft artificial labels as weight for unsupervised loss. 
2. Grouping similar soft artificial label values as additional batch and use different weight for each binning groups.   

Research Papers:

1. Co-Training
    1. Blum, Avrim, and Tom Mitchell. "Combining labeled and unlabeled data with co-training." *Proceedings of the eleventh annual conference on Computational learning theory*. 1998. [[pdf](https://dl.acm.org/doi/pdf/10.1145/279943.279962?casa_token=E4V63P1Pe0cAAAAA%3AfQYaS2xmV_h-xI60BVeWU5GRgs2eiY2k_UhQgvS2kHwfxKhOw_HrypPcfGUE6TaIzQ5_jRA0mN8dTQ)]

